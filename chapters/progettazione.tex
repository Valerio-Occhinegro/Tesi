\chapter{Progettazione e codifica}

\label{cap:progettazione}

\intro{Durante la fase di progettazione viene definita l'architettura del software, questa operazione consiste nella suddivisione del sistema in componenti distinti, ognuno con compiti differenti.
L'obiettivo è pianificare in maniera chiara tutte le azioni che l'applicativo dovrà svolgere prima di passare effettivamente alla codifica.}

\section{Architettura}
L'architettura (Fig.~\ref{fig:schema-architettura}) pensata prevede l'utilizzo di 4 componenti principali che si occupano di:
\begin{enumerate}
    \item Acquisire gli screenshot.
    \item Effettuare il clustering degli screenshot acquisiti.
    \item Valutare i siti web.
    \item Inviare e-mail promozionali.
\end{enumerate}
La suddivisione in componenti consente di avere un codice più strutturato e specializzato nell'assolvimento di determinati compiti; rende anche la comprensione da parte di altri programmatori più rapida e chiara.
I vari moduli comunicano con il database di SalesCRM attraverso l'utilizzo di API che eseguono interrogazioni e inserimenti.
Nel caso della comunicazione tra il modulo di clustering e quello di valutazione la condivisione di dati avviene in maniera diretta utilizzando la memoria dello stesso PC; questa modalità non si rivela problematica poiché garantisce uno scambio rapido di informazioni che deve essere svolto solo ed esclusivamente in fase di addestramento.

\begin{figure}[!h] 
    \centering 
    \includegraphics[width=0.9\columnwidth]{progettazione/schema-architettura.png} 
    \caption{Schema architetturale del progetto}
    \label{fig:schema-architettura}
  \end{figure}


\newpage

\section{Cattura immagini}
La prima fase del workflow (Fig.~\ref{fig:schema-cattura}) è composta da uno script Python che usufruisce della libreria Pyppeteer per acquisire gli screenshot dei siti contenuti nel database.
Più precisamente un web-scraper già implementato raccoglie i link delle pagine web dei clienti potenziali, successivamente li carica nel database di SalesCRM, dove verranno infine letti dallo script.
Per funzionare Pyppeteer necessita di chromium, dopo aver effettuato il controllo per verificare se esso sia presente o meno procede con la lettura dei link. 
L'automazione apre ogni indirizzo, aspetta qualche secondo e scatta uno screenshot. 
Tutte le immagini vengono poi convertite in formato Base64 e salvate nel database.
La conversione in Base64 viene sfruttata perché la codifica di un immagine in stringa consente il salvataggio diretto nelle tabelle.

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.5\columnwidth]{progettazione/schema-cattura.png} 
  \caption{Schema della fase di cattura}
  \label{fig:schema-cattura}
\end{figure}

\newpage

\section{Clustering}
\subsection{Preparazione delle immagini}
In seguito alla cattura le immagini vengono lette dal database e convertite in un formato utilizzabile dagli algoritmi di machine learning (Fig.~\ref{fig:schema-resize}).
%Quindi si passa da un formato Base64 a un formato raster che viene poi nuovamente convertito in un tensore multidimensionale adatto per essere processato.
Per velocizzare le operazioni in fase di addestramento tutte le immagini vengono scaricate dal database e convertite in formato PNG, poiché le librerie utilizzate necessitano di dati raster per funzionare, successivamente le immagini salvate nella memoria del PC vengono processate in maniera tale da essere adatte all'utilizzo da parte di ResNet50.
In questo caso le dimensioni dei tensori richieste specificatamente dal modello (Fig.~\ref{fig:schema-tensore}) sono le seguenti (224*224*3):
\begin{itemize}
  \item Altezza dell'immagine
  \item Larghezza dell'immagine 
  \item Numero di canali (Red Green Blue)
\end{itemize}
Per rientrare nel formato richiesto le immagini subiscono un processo di scaling che ne riduce la dimensionalità lasciando il contenuto invariato.
Tale operazione è stata preferita al cropping perché utilizzandolo andrebbero a perdersi caratteristiche specifiche; ad esempio i margini bianchi ai lati del sito sono spesso segnale di un design un po'antiquato e migliorabile,  
la rimozione dei margini potrebbe compromettere il clustering impedendo la creazione di un cluster contenente pagine di quel tipo.
Inoltre ogni cella contiene valori compresi tra 0 e 255, per facilitare le operazioni tali valori vengono divisi per 255 e normalizzati in un intervallo compreso tra 0 e 1.

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.7\columnwidth]{progettazione/schema-resize.png} 
  \caption{Conversione e ridimensionamento}
  \label{fig:schema-resize}
\end{figure}

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.5\columnwidth]{progettazione/schema-tensore.png} 
  \caption{Struttura del tensore}
  \label{fig:schema-tensore}
\end{figure}

\newpage


\subsection{Estrazione delle features}
Per ottenere un dato utilizzabile in modo efficace dall'algoritmo di clustering è necessario modificare ulteriormente le immagini riducendole a un set di features (Fig.~\ref{fig:featuremaps}).
Per adempire a questo compito viene utilizzato un CNN Convolutional Neural Network pre-addestrato chiamato ResNet50 (Fig.~\ref{fig:schema-resnet}).
Il modello viene caricato senza i top layers, ossia gli strati densi addetti alla classificazione, per sfruttare esclusivamente la sua abilità di riduzione in features.

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.7\columnwidth]{progettazione/esempio-featuremaps.png} 
  \caption{Esempio di featuremaps}
  \label{fig:featuremaps}
\end{figure}

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.9\columnwidth]{progettazione/schema-ResNet.png} 
  \caption{Struttura di ResNet50}
  \label{fig:schema-resnet}
\end{figure}



\subsection{Applicazione del clustering}
Le features estratte dai layers convoluzionali vengono ridotte a uno stato bidimensionale sfruttando l'analisi dei componenti principali, in questa maniera è possibile visualizzare i risultati del clustering su un grafico cartesiano, ridurre i tempi di calcolo e aumentare l'accuratezza delle predizioni; ma al costo di una ovvia perdita di informazioni.
Successivamente viene applicato l'algoritmo dei K-means per l'effettiva suddivisione del dataset in clusters (Fig.~\ref{fig:clusters-resnet}).
Le immagini vengono inserite, in base al cluster a loro assegnato, nelle rispettive cartelle di appartenenza.
Questa operazione viene svolta per semplificare lo step successivo in cui lo sviluppatore deve preparare manualmente il dataset per l'addestramento supervisionato.

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.7\columnwidth]{progettazione/Clusters-resnet.png} 
  \caption{Clustering effettuato utilizzando 20 clusters e 3000 immagini}
  \label{fig:clusters-resnet}
\end{figure}


%come modello finale è stato scelto resnet 50 poichè più rapido da utilizzare, infatti non necessita di addestramento 

\newpage

\section{Valutazione}

\subsection{Preparazione del dataset}
Il programmatore visiona i clusters ottenuti precedentemente e verifica quali possono appartenere alle categorie "siti migliorabili" e "siti ottimi"; prepara due cartelle corrispondenti alle categorie e inserisce le immagini che reputa appartenere a ciascun dominio.
Lo script prepara il dataset (Fig.~\ref{fig:struttura-dataset}) secondo la logica seguente:
\begin{itemize}
  \item Training, il 70\% delle immagini viene utilizzato per l'addestramento effettivo.
  \item Validation, il 15\% delle immagini viene utilizzato per ottimizzare i parametri del modello.
  \item Test, il 15\% delle immagini serve per valutare l'output del modello
\end{itemize}

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.35\columnwidth]{progettazione/struttura-dataset.png} 
  \caption{Struttura del dataset}
  \label{fig:struttura-dataset}
\end{figure}

\subsection{Addestramento del modello}
Viene caricato il modello pre-addestrato ResNet50 escludendo i top-layers, congelando i pesi e aggiungendo degli strati personalizzati per la classificazione.
Gli strati di classificazione sono successivamente addestrati utilizzando come input le features ricavate dai layers convoluzionali congelati di ResNet50. 

\subsection{Fine-tuning del modello}
I layers convoluzionali vengono sbloccati e il modello viene addestrato nuovamente nella sua interezza utilizzando un learning rate ridotto in maniera tale da non modificare completamente i pesi del modello pre-addestrato.
Il modello viene salvato e riutilizzato ogniqualvolta sia necessario.

\subsection{Valutazione delle immagini}
Le immagini presenti nel database vengono classificate dal modello e le valutazioni in centesimi vengono restituite.

\section{Invio e-mail}
Le valutazioni vengono lette dal database e tramite Laravel si procede all'invio di mail personalizzate alle aziende che dispongono di siti web che potrebbero essere ancora migliorati.

\section{Database}
Il database di SalesCRM contiene molte tabelle ma in questa sezione vengono descritte solo quelle utilizzate dal workflow.

\subsection{Domains}
Questa tabella contiene tutte i siti web (Fig.~\ref{fig:schema-domains}) delle aziende collezionate dal web-scraper e le informazioni a essi correlati.


\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.9\columnwidth]{progettazione/schema-domains.png} 
  \caption{Schema della tabella Domains}
  \label{fig:schema-domains}
\end{figure}


\subsection{Pages}
Contiene i link a tutte le pagine (Fig.~\ref{fig:schema-pages}) di ciascun dominio.

\begin{figure}[!h] 
  \centering 
  \includegraphics[width=0.9\columnwidth]{progettazione/schema-pages.png} 
  \caption{Schema della tabella Pages}
  \label{fig:schema-pages}
\end{figure}
